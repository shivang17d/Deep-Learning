{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a83105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lion', 'bird', 'rat', 'jungle'], ['monkey', 'sitting', 'tree'], ['rabbit', 'tree']]\n",
      "\n",
      "\n",
      "Similar words to 'jungle': [('sitting', 0.13919983804225922), ('monkey', 0.13157564401626587), ('bird', 0.06499496847391129), ('lion', 0.020040670409798622), ('rat', 0.010431213304400444)]\n",
      "Predicted word from context words ['lion', 'bird', 'rat']: sitting\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Data preparation\n",
    "def clean_and_tokenize(text):\n",
    "    # Tokenize sentences and words, remove punctuation, and convert to lowercase\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    word_tokens = [[word.lower() for word in sentence if word not in string.punctuation] for sentence in word_tokens]\n",
    "    return word_tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [[word for word in sentence if word.lower() not in stop_words] for sentence in tokens]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Combine tokenization and stopword removal for preprocessing\n",
    "    tokens = clean_and_tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return tokens\n",
    "\n",
    "# Generate training data\n",
    "def generate_cbow_data(tokens, window_size=2):\n",
    "    data = []\n",
    "    unique_words = set()\n",
    "\n",
    "    for sentence in tokens:\n",
    "        for i, target_word in enumerate(sentence):\n",
    "            # Generate context for each target word\n",
    "            context = [sentence[j] for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)) if j != i]\n",
    "            unique_words.update(context + [target_word])\n",
    "            data.append(context + [target_word])  # Combine context and target into a single list\n",
    "\n",
    "    return data, list(unique_words)\n",
    "\n",
    "# Train model\n",
    "def train_cbow_model(data, unique_words, embedding_size=100, epochs=100):\n",
    "    # Train Word2Vec CBOW model\n",
    "    model = Word2Vec(vector_size=embedding_size, window=2, sg=0, min_count=1)\n",
    "    model.build_vocab(data)\n",
    "    model.train(data, total_examples=model.corpus_count, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "# Output\n",
    "def find_similar_words(model, word, topn=5, exclude_words=None):\n",
    "    # Find similar words to a given word (excluding specified words)\n",
    "    similar_words = [(w, s) for w, s in model.wv.most_similar(word, topn=topn + len(exclude_words)) if w not in exclude_words]\n",
    "    return similar_words[:topn]\n",
    "\n",
    "def predict_word(model, context_words, exclude_words=None):\n",
    "    # Predict a word based on the context words provided\n",
    "    vector_sum = sum([model.wv[word] for word in context_words if word in model.wv and not all(char in string.punctuation for char in word) and word not in exclude_words])\n",
    "    if vector_sum.any():\n",
    "        # Find similar words to the calculated vector, excluding specified words and words in the context\n",
    "        filtered_similar_words = [(w, s) for w, s in model.wv.similar_by_vector(vector_sum, topn=len(model.wv)) if w not in exclude_words and w not in context_words]\n",
    "        return filtered_similar_words[0][0] if filtered_similar_words else None\n",
    "\n",
    "# Example text\n",
    "text = \"There was a lion, bird and rat in jungle. Monkey was sitting on tree. Rabbit was below the tree.\"\n",
    "tokens = preprocess_text(text)\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Generate CBOW training data\n",
    "cbow_data, unique_words = generate_cbow_data(tokens)\n",
    "\n",
    "# Train CBOW model\n",
    "cbow_model = train_cbow_model(cbow_data, unique_words, epochs=100)\n",
    "\n",
    "# Example output\n",
    "given_word = \"jungle\"\n",
    "exclude_punctuation = list(string.punctuation)\n",
    "similar_words = find_similar_words(cbow_model, given_word, exclude_words=exclude_punctuation)\n",
    "print(f\"Similar words to '{given_word}': {similar_words}\")\n",
    "\n",
    "example_context_words = ['lion', 'bird', 'rat']\n",
    "predicted_word = predict_word(cbow_model, example_context_words, exclude_words=exclude_punctuation)\n",
    "print(f\"Predicted word from context words {example_context_words}: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579df9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
