{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name : Shivang Gupta\n",
    "### Roll No : 3055\n",
    "### Deep Learning Assignment No : 5\n",
    "### Problem Statement : Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "####  a. Data preparation\n",
    "####  b. Generate training data\n",
    "####  c. Train model\n",
    "####  d. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necessary libraries and downloads NLTK resources (tokenizers and stopwords).\n",
    "Cleans and tokenizes a text into sentences and words.\n",
    "Removes stopwords and converts the text to lowercase.\n",
    "Trains a Word2Vec model on the tokenized text data.\n",
    "Finds similar words to a given word using the Word2Vec model.\n",
    "Selects a set of tokenized words and prepares a list of context-target word pairs.\n",
    "Predicts the current word from the context words using the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sCVfqccTjpe1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pL8XSt-9j-du",
    "outputId": "1f30e79a-2180-42cb-8dc9-f936f66b5e7f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "#Divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences\n",
    "nltk.download('stopwords')\n",
    "#Natural Language Toolkit(Stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nIlNgNVJkX9J"
   },
   "outputs": [],
   "source": [
    "sentences = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation Of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01) Cleaning Of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4n82Aq2Vk1pa",
    "outputId": "6eb5e040-f83e-4250-b8ff-21bf3fad929a"
   },
   "outputs": [],
   "source": [
    "#Remove one letter words\n",
    "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
    "\n",
    "#Remove special characters\n",
    "sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02) Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tgxIevXDnIdI"
   },
   "outputs": [],
   "source": [
    "#Convert all letters to lowercase\n",
    "sentences = sentences.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03) Tokenize sentences and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWGXxR0mm2Va",
    "outputId": "76998e5e-f72a-4240-fdbd-3ec4544907ce"
   },
   "outputs": [],
   "source": [
    "#Tokenize sentences (Tokenization is the act of breaking up a sequence of strings into pieces)\n",
    "all_sent=nltk.sent_tokenize(sentences)\n",
    "print(all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eD8aAL5knrSc"
   },
   "outputs": [],
   "source": [
    "#Break sentences into words\n",
    "all_words=[nltk.word_tokenize(sent) for sent in all_sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04) Removal Of Stopwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r_O3qiB5oa_U"
   },
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "  all_words[i]=[w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "data =all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-hjheB5ooGq",
    "outputId": "778c12c5-f32f-4435-cdf5-bbbe80788f4e"
   },
   "outputs": [],
   "source": [
    "#Training model using gensim\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,size = 52, window = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgfeXr8jovHn",
    "outputId": "e769b387-e4f0-4be1-9b17-f46002968ffc"
   },
   "outputs": [],
   "source": [
    "#Finding similar words to given word\n",
    "word='study'\n",
    "v1=model1.wv[word]\n",
    "similar_words=model1.wv.most_similar(word)\n",
    "for x in similar_words:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvQTxKJ2qfQW",
    "outputId": "74bcb035-0945-4438-d7ca-0b40b81b35b5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_data = data[0]\n",
    "print(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWLH1oMpq1Or",
    "outputId": "13040971-467f-4813-f12b-b23d36db1182"
   },
   "outputs": [],
   "source": [
    "#Preparing list of context words\n",
    "\n",
    "Context_Target_Data = []\n",
    "for i in range(2, len(split_data) - 2):\n",
    "    context = [split_data[i - 2], split_data[i - 1], split_data[i+1], split_data[i + 2]]\n",
    "    target = split_data[i]\n",
    "    Context_Target_Data.append((context, target))\n",
    "print(Context_Target_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qN_pKTDfr0PH",
    "outputId": "56cc8319-b0b6-4b65-c925-ebbe3a8d5e2a"
   },
   "outputs": [],
   "source": [
    "#Predicting current word from context words\n",
    "i=3\n",
    "print(Context_Target_Data[i][0],Context_Target_Data[i][1])\n",
    "print(model1.predict_output_word(Context_Target_Data[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
